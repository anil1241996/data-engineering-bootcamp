{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b5af885-04cd-48f1-be9a-d04f4a401529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ğŸ”¥ Apache Spark Fundamentals (Beginner to Expert)  \n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Spark Kya hai?  \n",
    "Apache Spark ek **Big Data processing engine** hai jo huge datasets ko **fast** aur **distributed computing** ke through process karta hai.  \n",
    "- Data ko ek hi machine pe nahi, balki **multiple machines (cluster)** me tod kar parallel process karta hai.  \n",
    "- Ye ETL, Data Analysis, Machine Learning aur Streaming sab me use hota hai.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Spark ki Core Entities  \n",
    "\n",
    "### ğŸ”¹ Driver Program  \n",
    "- Driver ek tarah ka **Manager / Boss** hai.  \n",
    "- Ye user code ko accept karta hai, usko **logical plan (DAG)** me todta hai, aur executors ko tasks assign karta hai.  \n",
    "- **Analogy:** Driver = Cricket team ka Captain.  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Executors  \n",
    "- Executors cluster ke **workers** hote hain jo actual computation karte hain.  \n",
    "- Driver se task lete hain aur data process karte hain.  \n",
    "- **Analogy:** Executors = Cricket Players.  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Cluster Manager  \n",
    "- Cluster Manager ka kaam hai **resources manage karna** (CPU, RAM, Executors).  \n",
    "- **Analogy:** Cluster Manager = Team Coach/Manager jo decide karta hai kaun player field me jayega.  \n",
    "\n",
    "ğŸ‘‰ Examples: YARN, Kubernetes, Mesos, Spark Standalone  \n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Spark Scheduling System  \n",
    "\n",
    "### ğŸ”¹ DAG Scheduler  \n",
    "- Spark job ko ek **DAG (Directed Acyclic Graph)** me todta hai.  \n",
    "- Job ko **Stages** me divide karta hai.  \n",
    "- **Analogy:** DAG Scheduler = Match ka Game Plan.  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Task Scheduler  \n",
    "- Stages ko **tasks** me todta hai.  \n",
    "- Tasks ko executors pe assign karta hai.  \n",
    "- **Analogy:** Task Scheduler = Captain ka batting/bowling order.  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Tasks  \n",
    "- Tasks Spark ka sabse chhota **unit of work** hote hain.  \n",
    "- Har task ek partition process karta hai.  \n",
    "- **Analogy:** Task = Player ka individual role.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ # âš¡ Apache Spark Execution Flow (Detailed)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Step by Step Execution Flow\n",
    "\n",
    "```text\n",
    "   User Code (PySpark, Scala, SQL)\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â–ª Tum Spark me DataFrame / RDD operations likhte ho\n",
    "   â–ª Example: df.groupBy(\"city\").count()\n",
    "             â”‚\n",
    "             â–¼\n",
    "   Driver Program\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â–ª User code ko receive karta hai\n",
    "   â–ª Uska Logical Plan banata hai\n",
    "   â–ª Logical Plan â†’ Optimizer â†’ Physical Plan\n",
    "             â”‚\n",
    "             â–¼\n",
    "   DAG Scheduler\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â–ª Physical Plan ko DAG (Directed Acyclic Graph) me todta hai\n",
    "   â–ª Job ko multiple Stages me todta hai\n",
    "     - Shuffle boundaries pe stage break hota hai\n",
    "   â–ª Example:\n",
    "       Stage 1 â†’ Read + Map\n",
    "       Stage 2 â†’ Shuffle + Reduce\n",
    "             â”‚\n",
    "             â–¼\n",
    "   Task Scheduler\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â–ª Har Stage ko chhote **Tasks** me todta hai\n",
    "   â–ª Task = Partition level work unit\n",
    "   â–ª Executors ko Task bhejne ka kaam karta hai\n",
    "             â”‚\n",
    "             â–¼\n",
    "   Cluster Manager\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â–ª Executors ke liye resources allocate karta hai\n",
    "   â–ª Example: YARN, Kubernetes, Mesos\n",
    "             â”‚\n",
    "             â–¼\n",
    "   Executors (Workers)\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â–ª Har Task ko execute karte hain (per partition)\n",
    "   â–ª Parallel computation hoti hai across cluster\n",
    "   â–ª Intermediate shuffle data exchange bhi yahi hoti hai\n",
    "             â”‚\n",
    "             â–¼\n",
    "   Driver Collects Results\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â–ª Executors se results leke\n",
    "   â–ª Tumhare Spark Session ya Storage (HDFS, Delta, S3, DB) me save kar deta hai\n",
    "\n",
    " **_âš¡ Apache Spark Execution Flow_**\n",
    "\n",
    "```text\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚   User Code (PySpark/SQL)  â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚       Driver Program        â”‚\n",
    "         â”‚ Logical & Physical Plan     â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚        DAG Scheduler        â”‚\n",
    "         â”‚ Job â†’ Stages (Shuffle Cuts) â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚       Task Scheduler        â”‚\n",
    "         â”‚ Stages â†’ Tasks (per-part)  â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚     Cluster Manager         â”‚\n",
    "         â”‚ Allocates CPU / RAM / Execs â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Executor 1    â”‚   â”‚ Executor 2    â”‚   â”‚ Executor N    â”‚\n",
    "   â”‚ Run Tasks     â”‚   â”‚ Run Tasks     â”‚   â”‚ Run Tasks     â”‚\n",
    "   â”‚ Store Results â”‚   â”‚ Store Results â”‚   â”‚ Store Results â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚  Driver Collects Results    â”‚\n",
    "         â”‚  â†’ Output / Storage         â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "300c6d75-d517-41c0-a561-1359806959e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('hello world')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apache Spark Architecture",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
