{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b5af885-04cd-48f1-be9a-d04f4a401529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🔥 Apache Spark Fundamentals (Beginner to Expert)  \n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Spark Kya hai?  \n",
    "Apache Spark ek **Big Data processing engine** hai jo huge datasets ko **fast** aur **distributed computing** ke through process karta hai.  \n",
    "- Data ko ek hi machine pe nahi, balki **multiple machines (cluster)** me tod kar parallel process karta hai.  \n",
    "- Ye ETL, Data Analysis, Machine Learning aur Streaming sab me use hota hai.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Spark ki Core Entities  \n",
    "\n",
    "### 🔹 Driver Program  \n",
    "- Driver ek tarah ka **Manager / Boss** hai.  \n",
    "- Ye user code ko accept karta hai, usko **logical plan (DAG)** me todta hai, aur executors ko tasks assign karta hai.  \n",
    "- **Analogy:** Driver = Cricket team ka Captain.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Executors  \n",
    "- Executors cluster ke **workers** hote hain jo actual computation karte hain.  \n",
    "- Driver se task lete hain aur data process karte hain.  \n",
    "- **Analogy:** Executors = Cricket Players.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Cluster Manager  \n",
    "- Cluster Manager ka kaam hai **resources manage karna** (CPU, RAM, Executors).  \n",
    "- **Analogy:** Cluster Manager = Team Coach/Manager jo decide karta hai kaun player field me jayega.  \n",
    "\n",
    "👉 Examples: YARN, Kubernetes, Mesos, Spark Standalone  \n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Spark Scheduling System  \n",
    "\n",
    "### 🔹 DAG Scheduler  \n",
    "- Spark job ko ek **DAG (Directed Acyclic Graph)** me todta hai.  \n",
    "- Job ko **Stages** me divide karta hai.  \n",
    "- **Analogy:** DAG Scheduler = Match ka Game Plan.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Task Scheduler  \n",
    "- Stages ko **tasks** me todta hai.  \n",
    "- Tasks ko executors pe assign karta hai.  \n",
    "- **Analogy:** Task Scheduler = Captain ka batting/bowling order.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Tasks  \n",
    "- Tasks Spark ka sabse chhota **unit of work** hote hain.  \n",
    "- Har task ek partition process karta hai.  \n",
    "- **Analogy:** Task = Player ka individual role.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ # ⚡ Apache Spark Execution Flow (Detailed)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Step by Step Execution Flow\n",
    "\n",
    "```text\n",
    "   User Code (PySpark, Scala, SQL)\n",
    "   ───────────────────────────────\n",
    "   ▪ Tum Spark me DataFrame / RDD operations likhte ho\n",
    "   ▪ Example: df.groupBy(\"city\").count()\n",
    "             │\n",
    "             ▼\n",
    "   Driver Program\n",
    "   ───────────────────────────────\n",
    "   ▪ User code ko receive karta hai\n",
    "   ▪ Uska Logical Plan banata hai\n",
    "   ▪ Logical Plan → Optimizer → Physical Plan\n",
    "             │\n",
    "             ▼\n",
    "   DAG Scheduler\n",
    "   ───────────────────────────────\n",
    "   ▪ Physical Plan ko DAG (Directed Acyclic Graph) me todta hai\n",
    "   ▪ Job ko multiple Stages me todta hai\n",
    "     - Shuffle boundaries pe stage break hota hai\n",
    "   ▪ Example:\n",
    "       Stage 1 → Read + Map\n",
    "       Stage 2 → Shuffle + Reduce\n",
    "             │\n",
    "             ▼\n",
    "   Task Scheduler\n",
    "   ───────────────────────────────\n",
    "   ▪ Har Stage ko chhote **Tasks** me todta hai\n",
    "   ▪ Task = Partition level work unit\n",
    "   ▪ Executors ko Task bhejne ka kaam karta hai\n",
    "             │\n",
    "             ▼\n",
    "   Cluster Manager\n",
    "   ───────────────────────────────\n",
    "   ▪ Executors ke liye resources allocate karta hai\n",
    "   ▪ Example: YARN, Kubernetes, Mesos\n",
    "             │\n",
    "             ▼\n",
    "   Executors (Workers)\n",
    "   ───────────────────────────────\n",
    "   ▪ Har Task ko execute karte hain (per partition)\n",
    "   ▪ Parallel computation hoti hai across cluster\n",
    "   ▪ Intermediate shuffle data exchange bhi yahi hoti hai\n",
    "             │\n",
    "             ▼\n",
    "   Driver Collects Results\n",
    "   ───────────────────────────────\n",
    "   ▪ Executors se results leke\n",
    "   ▪ Tumhare Spark Session ya Storage (HDFS, Delta, S3, DB) me save kar deta hai\n",
    "\n",
    " **_⚡ Apache Spark Execution Flow_**\n",
    "\n",
    "```text\n",
    "         ┌────────────────────────────┐\n",
    "         │   User Code (PySpark/SQL)  │\n",
    "         └──────────────┬─────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "         ┌────────────────────────────┐\n",
    "         │       Driver Program        │\n",
    "         │ Logical & Physical Plan     │\n",
    "         └──────────────┬─────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "         ┌────────────────────────────┐\n",
    "         │        DAG Scheduler        │\n",
    "         │ Job → Stages (Shuffle Cuts) │\n",
    "         └──────────────┬─────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "         ┌────────────────────────────┐\n",
    "         │       Task Scheduler        │\n",
    "         │ Stages → Tasks (per-part)  │\n",
    "         └──────────────┬─────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "         ┌────────────────────────────┐\n",
    "         │     Cluster Manager         │\n",
    "         │ Allocates CPU / RAM / Execs │\n",
    "         └──────────────┬─────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "   ┌───────────────┐   ┌───────────────┐   ┌───────────────┐\n",
    "   │ Executor 1    │   │ Executor 2    │   │ Executor N    │\n",
    "   │ Run Tasks     │   │ Run Tasks     │   │ Run Tasks     │\n",
    "   │ Store Results │   │ Store Results │   │ Store Results │\n",
    "   └───────────────┘   └───────────────┘   └───────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "         ┌────────────────────────────┐\n",
    "         │  Driver Collects Results    │\n",
    "         │  → Output / Storage         │\n",
    "         └────────────────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "300c6d75-d517-41c0-a561-1359806959e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('hello world')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apache Spark Architecture",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
