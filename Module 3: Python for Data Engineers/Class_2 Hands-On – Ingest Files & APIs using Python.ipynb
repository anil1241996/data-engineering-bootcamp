{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5db78f9-df17-4ea4-9d98-f328e409cbb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**_File Handling_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478e64be-c498-48f9-b93e-d5dead7ba47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /Volumes/workspace/default/dbfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "727a338b-61f4-42e3-9bb3-fae93fe6bc48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using csv file create dataframe then save as sql table\n",
    "\n",
    "#df = spark.read.option(\"header\", \"true\") \\\n",
    "#               .option(\"inferSchema\", \"true\") \\\n",
    "#               .option(\"multiline\", \"true\") \\\n",
    "#               .csv(\"/Volumes/workspace/default/dbfs/global_food_wastage_dataset.csv\")\n",
    "\n",
    "#df.write.saveAsTable(\"global_food_wastage_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bb50cf-97a7-474b-ac3a-88441bd63c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic Read the CSV File\n",
    "\n",
    "import csv\n",
    "with open('/Volumes/workspace/default/dbfs/global_food_wastage_dataset.csv', mode = 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    # next is used when you want to skip the header from csv file \n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51fecac0-d3de-47a7-b6a5-5b8633bf0122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic Read the CSV File\n",
    "# Only some part of the file \n",
    "\n",
    "import csv\n",
    "\n",
    "with open('/Volumes/workspace/default/dbfs/global_food_wastage_dataset.csv', mode = 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db57b91-61e5-40b3-8862-ad93e9babb02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic Read the CSV File\n",
    "# Only some part of the file \n",
    "\n",
    "import csv\n",
    "\n",
    "with open('/Volumes/workspace/default/dbfs/global_food_wastage_dataset.csv', mode = 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "    for row in reader:\n",
    "        print(row['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45cd8b93-a053-4d95-b6e8-0b2353362c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing to CSV File\n",
    "\n",
    "with open('out.csv', mode = 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Country', 'Wastage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a1ba81-60e7-4d45-bfa5-938c15e5e587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('log.txt', mode = 'r') as log_file:\n",
    "    for line in log_file:\n",
    "        print(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d306f6be-cd90-4fc7-9f26-9c28a1bf9393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('log.txt', mode = 'r') as log_file:\n",
    "    for line in log_file:\n",
    "        if 'ERROR' in line:\n",
    "            print(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a769ab1-e71e-4b27-9c5a-22d82c0d9dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetching Data From API\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://myfitnesspal2.p.rapidapi.com/searchByKeyword\"\n",
    "\n",
    "querystring = {\"keyword\":\"oreo\",\"page\":\"1\"}\n",
    "\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"98cb4e492amshbd8e648756566d1p1580cdjsn84dd40ac9a57\",\n",
    "\t\"x-rapidapi-host\": \"myfitnesspal2.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2717d06-bf11-4c5f-b32b-c9500def57e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://cricbuzz-cricket.p.rapidapi.com/matches/v1/recent\"\n",
    "\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"98cb4e492amshbd8e648756566d1p1580cdjsn84dd40ac9a57\",\n",
    "\t\"x-rapidapi-host\": \"cricbuzz-cricket.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0644c1ea-c3eb-4eb5-a164-47f806f9d078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://cricbuzz-cricket.p.rapidapi.com/matches/v1/recent\"\n",
    "\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"98cb4e492amshbd8e648756566d1p1580cdjsn84dd40ac9a57\",\n",
    "\t\"x-rapidapi-host\": \"cricbuzz-cricket.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.json(['typeMatches']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64f129c9-d957-4c19-8672-9b14256985e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Assignment 2: Reading Files & Handling Errors Like a Real Engineer\n",
    "\n",
    "_**Description:**_\n",
    "\n",
    "You'll simulate file ingestion and error-proof reading of customer records - a task every data engineer handles when working with logs, CSVs, or batch files.\n",
    "\n",
    "**_Objective:_**\n",
    "\n",
    "To gain confidence in reading/writing files, handling missing data, and managing exceptions without breaking the code flow.\n",
    "\n",
    "**_Tasks:_**\n",
    "\n",
    "1. Read a CSV file (`sample_customers.csv`) using the built-in `csv` module and print each row.\n",
    "2. Implement a `try-except` block that handles:\n",
    "- FileNotFoundError\n",
    "- ValueError during row parsing\n",
    "3. Append a new customer record to the same CSV file using file write mode (`'a'`).\n",
    "4. Log success and error messages using basic `print()` or `logging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc91927-8a71-43b2-83b2-9728b49aece8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import csv file - sample_customers\n",
    "\n",
    "import csv\n",
    "with open('/Volumes/workspace/default/dbfs/sample_customers.csv', mode = 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4651f1-3b16-481c-bd34-85dcdfcdaa5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import csv file using try and except login for error handling\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    with open('/Volumes/workspace/default/dbfs/sample_customers.csv', mode = 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                print(row)\n",
    "            except ValueError as ve:\n",
    "                print(f\"ValueError encountered: {ve}\")\n",
    "except FileNotFoundError as fnf_error:\n",
    "    print(f\"FileNotFoundError: {fnf_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051bb6ea-a921-43c1-9622-e2c6fc42f9b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import csv file - sample_orders \n",
    "\n",
    "import json\n",
    "with open('/Volumes/workspace/default/dbfs/sample_orders.json', mode = 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148d70bb-23eb-4ff2-890b-696f9cdf6f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read sample_customers.csv into a DataFrame\n",
    "df_customers = spark.read.option(\"header\", \"true\").csv(\"/Volumes/workspace/default/dbfs/sample_customers.csv\")\n",
    "\n",
    "# Read sample_orders.json into a DataFrame\n",
    "df_orders = spark.read.option(\"multiline\", \"true\").json(\"/Volumes/workspace/default/dbfs/sample_orders.json\")\n",
    "\n",
    "# Join both DataFrames on customer_id\n",
    "df_joined = df_customers.join(df_orders, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Display the joined DataFrame\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1894ff87-aca3-470c-ad49-01baea0531ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**_Hands-On – Make Pipelines Reliable with Error Handling & Logging_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caefe391-8405-4222-a9c6-e6b4f987fa9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# try - except\n",
    "\n",
    "try:\n",
    "    print('hello world')\n",
    "except:\n",
    "    print('fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1b2d9a-cb8f-4a8c-b098-a47f6e320896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Catch Multiple Error:\n",
    "try:\n",
    "    marks = int(input('Enter the number : '))\n",
    "    print('Normalized Score : ', 100/marks)\n",
    "except ZeroDivisionError:\n",
    "    print('Cannot divided by zero')\n",
    "except ValueError:\n",
    "    print('Please enter valid number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ae9434-577d-4c98-bd5b-dd072678005a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Catch Multiple Error + Use If/Else/Finally\n",
    "#import round\n",
    "try:\n",
    "    a = int(input('Enter the number A: '))\n",
    "    b = int(input('Enter the number B: '))\n",
    "    result = round(a/b, 2)\n",
    "\n",
    "except ZeroDivisionError:\n",
    "    print('Cannot divided by zero')\n",
    "\n",
    "except ValueError:\n",
    "    print('Please enter valid number')\n",
    "\n",
    "else:\n",
    "    print('Result: ', result)\n",
    "\n",
    "finally:\n",
    "    print('Calculator Task Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c0b0809-8ba9-41f8-b009-64f4a8598936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How would you handle if someone enters a string for age in a form?\n",
    "\n",
    "age = input('Enter Age: ')\n",
    "if age.isdigit():\n",
    "    print('Valid Age: ', age)\n",
    "else:\n",
    "    print('Invalid Age - Must be the Number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dec1868e-1ab3-4d7a-992f-86799609553c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**_Into to Logging (Real-World Tracking_**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667a00c7-6a99-450b-9a71-5095cc1945b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "logging.debug('Debug - Internal Variable Set')\n",
    "logging.info('Infor - Starting Process')\n",
    "logging.warning('Warning - Using Default Configuration')\n",
    "logging.error('Error - Invalid Input Received')\n",
    "logging.critical('Critical - System Failure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4526de-36c2-4c7c-9e82-808818618ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "create table LOGGING_INFO(\n",
    "  name varchar(50),\n",
    "  status varchar(20),\n",
    "  date varchar(50)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe809a8-1242-4da9-b0c5-9ec1233c87f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from LOGGING_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8999f255-5014-40d0-9ed8-2a3d642f1675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_status:\n",
    "    # some update statement\n",
    "\n",
    "def insert_status:\n",
    "    # some insert statement\n",
    "\n",
    "def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "126364fb-f569-4e56-bdd9-5b9cb09d39f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📌 Why Data Engineers Need OOPs Concept\n",
    "\n",
    "## 🔹 OOPs Basics\n",
    "OOPs (Object Oriented Programming System) ek programming style hai jisme code ko **classes aur objects** ke form me likha jata hai.  \n",
    "Iske main 4 concepts hote hain:\n",
    "\n",
    "1. **Encapsulation** – Data aur methods ko ek class me band karna  \n",
    "2. **Inheritance** – Ek class ka behavior dusre class me reuse karna  \n",
    "3. **Polymorphism** – Same function ka alag-alag behavior  \n",
    "4. **Abstraction** – Sirf zaroori details dikhana, baaki hide karna  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Data Engineers ko OOPs kyun chahiye?\n",
    "\n",
    "### 1. **Reusable Code**\n",
    "Har baar naya pipeline likhne ki jagah, ek **class bana lo** aur usse multiple projects me reuse karo.  \n",
    "👉 Example: Ek `CsvReader` class bana kar har jagah use kar sakte ho.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Maintainability & Clean Code**\n",
    "Data projects bahut bade hote hain. Agar sab kuch functions me likhoge to code messy ho jayega.  \n",
    "Classes ke andar likhne se code **organized aur readable** rahta hai.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Scalability**\n",
    "Pipeline me naye features add karna easy ho jata hai.  \n",
    "👉 Example: Agar `FileReader` class hai, to tum easily JSON ya Parquet ka support add kar sakte ho.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Team Collaboration**\n",
    "Team projects me OOPs ek **standard structure** deta hai jisse dusre engineers ke liye samajhna easy ho jata hai.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Integration with Big Data Tools**\n",
    "PySpark, Databricks, Airflow, ADF jaise tools already OOPs concepts pe based hote hain.  \n",
    "👉 Example: PySpark me `DataFrame` ek class hai jisme `.filter()`, `.groupBy()` jaise methods defined hote hain.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Example: ETL Pipeline using OOPs\n",
    "\n",
    "```python\n",
    "class DataExtractor:\n",
    "    def __init__(self, source):\n",
    "        self.source = source\n",
    "    \n",
    "    def extract(self):\n",
    "        print(f\"Extracting data from {self.source}\")\n",
    "        return \"sample data\"\n",
    "\n",
    "class DataTransformer:\n",
    "    def transform(self, data):\n",
    "        print(f\"Transforming {data}\")\n",
    "        return data.upper()\n",
    "\n",
    "class DataLoader:\n",
    "    def load(self, data):\n",
    "        print(f\"Loading data: {data}\")\n",
    "\n",
    "\n",
    "# Pipeline Execution\n",
    "extractor = DataExtractor(\"CSV File\")\n",
    "data = extractor.extract()\n",
    "\n",
    "transformer = DataTransformer()\n",
    "transformed = transformer.transform(data)\n",
    "\n",
    "loader = DataLoader()\n",
    "loader.load(transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce2e822-b572-45c7-80ae-6bf3f9b180e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dict_sam = {'a':1, 'b':2, 'c':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52271b3c-6a99-43ab-a903-b169939a4b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(dict_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03330913-13ca-4fc3-8d0b-b54a85dfff0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[1, 2 ,3] + [4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4c1227-bc03-475b-a960-e6e0ba7ed088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'data' * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b30c3e-fcc2-4f6f-b3a3-ecca873a6383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type([]) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b8c2f6-c0fb-44bd-a7d0-56c87a05b25c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'DataX'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bf305b4-f89a-4959-a19d-f3f13a090846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e32a2f7-32bd-479d-8285-6c86ff4d7637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bool([])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6920212331048466,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Class_2 Hands-On – Ingest Files & APIs using Python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
